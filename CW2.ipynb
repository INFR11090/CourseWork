{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # avoid non-GUI warning for matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from itertools import product as cartesian_product\n",
    "from skimage.draw import circle, circle_perimeter\n",
    "\n",
    "from utils import MazeEnv\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(linewidth=250,precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeGenerator(object):\n",
    "    def __init__(self):\n",
    "        self.maze = None\n",
    "    \n",
    "    def init_end_states(self):\n",
    "        \"\"\"Get start and end goals\"\"\"\n",
    "        init_state = [3,1]\n",
    "        goal_states = [[11,17]]\n",
    "        return init_state, goal_states\n",
    "        \n",
    "    def get_maze(self):\n",
    "        return self.maze\n",
    "\n",
    "\n",
    "class SimpleMazeGenerator(MazeGenerator):\n",
    "    def __init__(self, maze):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.maze = maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Region ID's (also related to the region colours, more in utils.py - line 70)\n",
    "regions = {'water':6, 'edge':7, 'sky':8, 'mine':4, 'uncharted':5}\n",
    "\n",
    "env_size = 19\n",
    "maze_array = np.zeros([env_size,env_size]) + regions['water']\n",
    "\n",
    "#Boundary\n",
    "maze_array[:,0] = regions['edge']\n",
    "maze_array[:,-1] = regions['edge']\n",
    "maze_array[-1,:] = regions['edge']\n",
    "maze_array[0:2,:] = regions['sky']\n",
    "\n",
    "#Bombs\n",
    "j = 4\n",
    "maze_array[circle(11,3,2)] = regions['mine']\n",
    "maze_array[circle(6,14,2)] = regions['mine']\n",
    "maze_array[circle(15,4,2)] = regions['mine']\n",
    "maze_array[circle(5,10,2)] = regions['mine']\n",
    "maze_array[circle(13,16,2)] = regions['mine']\n",
    "maze_array[circle(4,4,2)] = regions['mine']\n",
    "\n",
    "#Unmapped territory\n",
    "maze_array[circle(12,10,5)] = regions['uncharted']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the underwater maze environment\n",
    "maze = SimpleMazeGenerator(maze_array)\n",
    "submarine = MazeEnv(maze, render_trace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6adfde4940>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6adfe8eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A view of the environment\n",
    "\n",
    "#reset(), resets the environment back to it's initial conditions and returns a view of it\n",
    "snapshot = submarine.reset() \n",
    "\n",
    "plt.imshow(snapshot, cmap=submarine.cmap, norm=submarine.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Actions :  4 \n",
      "Action ID: Up, Down, Left, Right =  [0, 1, 2, 3] \n",
      "Goal states:  [[11, 17]] \n",
      "Maze size:  (19, 19) \n",
      "List of all states in the environment:  [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4)] ...\n"
     ]
    }
   ],
   "source": [
    "# Some available information from the environment\n",
    "print(\"Number of Actions : \", submarine.num_actions,\n",
    "      \"\\nAction ID: Up, Down, Left, Right = \",submarine.all_actions,\n",
    "     \"\\nGoal states: \", submarine.goal_states,\n",
    "      \"\\nMaze size: \", submarine.maze_size,\n",
    "     \"\\nList of all states in the environment: \", submarine.valid_states[0:5], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6adfd427b8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6adfdba978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Unlike the traditional gym environment, each call to the step() function executes an action and takes a step through the dynamics of the environment.\n",
    "#The resulting environmental observation is returned. Note: This function updates the state of the executing agent.\n",
    "\n",
    "submarine.step(1)\n",
    "submarine.step(1)\n",
    "submarine.step(1)\n",
    "snapshot = submarine.step(1)\n",
    "plt.imshow(snapshot, cmap=submarine.cmap, norm=submarine.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6adfd240f0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6adfd5ceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submarine.step(3)\n",
    "submarine.step(3)\n",
    "snapshot = submarine.step(3)\n",
    "plt.imshow(snapshot, cmap=submarine.cmap, norm=submarine.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _next_state(current_state,action) takes a state and an action as an input and returns the resulting state.\n",
    "# Note: This function does not update the state of the executing agent.\n",
    "submarine._next_state((0,0),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CourseWork\n",
    "\n",
    "You are to design an agent that will be in charge of the navigation of a submarine, through a\n",
    "minefield, towards a goal.\n",
    "\n",
    "Navigating through uncharted territory can be dangerous, which we will model by applying some damage to the submarine. Additionally, there is some uncertainty~ in the execution of actions when moving through uncharted territory. Currents are very strong and there is a 20% chance that the submarine will diverge away from its path during execution. For example, when going down within the uncharted territory, there is an 80% chance it will go down, but also a 10% chance that it will go left, and another 10% chance it will go right.\n",
    "\n",
    "\n",
    "Hitting a mine will lead to the destruction of the submarine and boundary regions marked **edge** or **sky** cannot be traversed.  Your task is to develop a strategy for this agent (plotted as the *blue* dot) to safely navigate towards the goal (plotted as the *green* dot).\n",
    "\n",
    "You are to do this using the principles of Dynamic Programming. In particular, in this exercise,\n",
    "you will implement the methods of Policy and Value Iteration. For all of these questions, please use\n",
    "the provided visualisation functions to plot your policies and their associated trajectories. Include\n",
    "concise comments within the notebook to highlight any interesting observations to support your\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 : Reward and Transition Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define suitable reward and transition functions for the submarine environment. This is a key task for you as the\n",
    "designer of this agent. \n",
    "\n",
    "The reward function specifies a scalar reward for every state in the environment (in accordance to the coursework description above) and the transition function determines the next *feasible* state, $s'$, reached when an action $a$ is taken from state $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0., -1., -1., -1.,  0., -1., -1., -1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1.,  0., -1., -1., -1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1.,  0.],\n",
       "       [ 0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1.,  0.],\n",
       "       [ 0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HINT: Example reward function\n",
    "R = np.zeros_like(submarine.maze)\n",
    "R -= (1*(maze_array == regions['mine']).astype('f'))\n",
    "R[submarine.goal_states[0][0],submarine.goal_states[0][1]] = 1\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2: Value Iteration\n",
    "\n",
    "\n",
    "Your first algorithmic task will be to apply value iteration to the solution of this navigation task, using the reward and transition functions you have devised in the previous step within the given the specification of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: Example Iteration Template\n",
    "V = np.zeros_like(submarine.maze)\n",
    "V_sum = 0\n",
    "discount = 0.8\n",
    "for i in range(10000):\n",
    "    for state, region in np.ndenumerate(submarine.maze):\n",
    "        \n",
    "        #Bellman Update\n",
    "        \n",
    "        \n",
    "    if (i % 10) == 0:\n",
    "        if np.abs(V.sum() - V_sum) < 1e-6:\n",
    "            print(f'Value iteration converged at iteration #{i+1:,}')\n",
    "            break\n",
    "        V_sum = V.sum()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(V,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HINT: You will also need to create a deterministic policy using the optimal value function\n",
    "policy = np.zeros_like(V)\n",
    "\n",
    "for state, region in np.ndenumerate(submarine.maze):\n",
    "    action_values = np.zeros(shape=submarine.num_actions)\n",
    "    for action in range(submarine.num_actions):\n",
    "        next_state = tuple(submarine._next_state(state,action))\n",
    "        \n",
    "    #select the best action based on the highest state-action value\n",
    "    policy[state] = np.argmax(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should visulise some trajectories from a few different initial points within the environment (e.g. [23,23], [23,5], [6,4],[6,23])\n",
    "submarine.init_state = [5,23] #[23,2] [4,23]\n",
    "submarine.reset()\n",
    "state = submarine.init_state\n",
    "indx = 0\n",
    "while state not in submarine.goal_states:\n",
    "    submarine.step(policy[tuple(state)])\n",
    "    fig = submarine.render()\n",
    "    state = submarine.state\n",
    "    \n",
    "    if indx > 1000:\n",
    "        print(\"Max Iteration Limit Hit!\")\n",
    "        break\n",
    "    indx += 1\n",
    "anime = submarine._get_video(interval=200, gif_path='value_iter.mp4').to_html5_video()\n",
    "HTML(anime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectories can also be visualied via the simulate function \n",
    "# (note: function buggy on some systems; needs to be run twice. )\n",
    "HTML(simulate(submarine, [23,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot the final policy\n",
    "draw_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3: Policy Evaluation\n",
    "    \n",
    "Define a function that computes the state-value function $V^\\pi$ on this environment when given a random policy $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4: Policy Iteration\n",
    "Similarly to the earlier section, now apply policy iteration to solving the same navigation task. In\n",
    "what ways are the results of policy iteration different from those of value iteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.5: Hyper-parameters\n",
    "\n",
    "For both policy and value iteration:\n",
    "- How does varying the discount factor affect this calculated policy? Repeat the same experiment with at least three different settings of a discount factor in order to make your argument.\n",
    "- Given the insights from your experiment above, suggest a suitable strategy for setting an apropriate discount rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.6: Noisy trigger\n",
    "\n",
    "The mines now have a 50% chance of blowing up if you are within their direct vicinity (i.e. on their border). \n",
    "\n",
    "How does this affect your policy? \n",
    "\n",
    "Implement this and justify your answer based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "*Remark: This second question need not be implemented in the notebook, in the same way you\n",
    "wrote programs for the first question. Instead, we expect to see a complete problem formulation,\n",
    "with suitable expressions and graphs, and an explanation of the solution procedure in terms of those\n",
    "expressions.*\n",
    "\n",
    "\n",
    "Now consider two additions to the above problem specification that are common in realistic versions\n",
    "of this problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1. \n",
    "The agent must include within the costs, the ides of navigability - describing the fact that some parts of a given map are easier to travel through than others (typically, due to the ‘difficulty’ of the terrain features at that depth). One way to model this would be to include a distribution of costs over the given terrain map. By re-writing your problem specification in complete terms, explain how you will incorporate this feature in your modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2. \n",
    "The agents in the environment, previously viewed as static obstacles defining where you can and can not traverse, could have their own dynamics. For instance, one of the mines could be an active craft with its own motion policy. \n",
    "\n",
    "Assuming that you can observe the current position of this active craft through a noisy channel (e.g., sonar reflections), pose your - now interactive - motion planning problem in Bayesian terms and explain how your solution strategy might need to be altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
